{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***scalar.py***\n",
    "\n",
    "Il programma è strutturato principalmente in due classi:\n",
    "\n",
    "* *ScalarModel*\n",
    "* *TabularLearner*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *ScalarModel*\n",
    "\n",
    "* **Inizializzazione**: \n",
    "    ``` python\n",
    "    def __init__(self, reward, draw=False)\n",
    "    ```\n",
    "    Questa classe modella l'ambiente cellulare e gestisce il comportamento delle cellule nel tempo. Il costruttore inizializza il modello con il tipo di ricompensa (*reward*) e se disegnare o meno il grafico dell'evoluzione delle cellule (*draw*).\n",
    "\n",
    "* **Reset**: \n",
    "    ``` python\n",
    "    def reset(self)\n",
    "    ```\n",
    "    La funzione *reset* reimposta lo stato dell'ambiente cellulare, inizializzando il tempo, la quantità di glucosio e ossigeno, e crea una popolazione di cellule sane e cancerose. Con la riga seguente crea una lista contenente una cellula cancerosa e 1000 cellule sane. Tutti gli oggetti \"cellula\", sono creati passando il parametro 0. PERCHE? COME MAI CREO SOLO UNA CELLULA CANCEROSA?\n",
    "    ``` python\n",
    "    self.cells = [CancerCell(0)] + [HealthyCell(0) for _ in range(1000)]\n",
    "    ```\n",
    "\n",
    "* **Ciclo delle cellule**:\n",
    "    ``` python\n",
    "    def cycle_cells(self):\n",
    "        ...\n",
    "    ```\n",
    "    Questa funzione simula il ciclo di vita delle cellule, dove ogni cellula consuma risorse e può dividersi (mitosi) se le condizioni lo permettono.\n",
    "\n",
    "    *   ``` python\n",
    "        to_add = []\n",
    "        ```\n",
    "        La lista *to_add* viene utilizzata per raccogliere le nuove cellule che si formeranno durante il ciclo.\n",
    "\n",
    "    *   ``` python\n",
    "        for cell in random.sample(self.cells, count):\n",
    "        ```\n",
    "        La funzione itera su un campione casuale di cellule. *random.sample(self.cells, count)* restituisce un campione di count cellule dall'elenco delle cellule attuali.\n",
    "\n",
    "\n",
    "    *   ``` python\n",
    "        res = cell.cycle(self.glucose, count / 278 , self.oxygen)\n",
    "        ```\n",
    "        Ogni cellula esegue il proprio ciclo di vita chiamando il metodo *cycle* della cellula stessa. Questo metodo riceve come parametri la quantità di glucosio disponibile, una frazione del numero totale di cellule (*count / 278*), e la quantità di ossigeno disponibile. *res* è una lista che contiene: Il consumo di glucosio della cellula, Il consumo di glucosio della cellula e un terzo valore opzionale che indica se la cellula si è divisa (mitosi) e il tipo di cellula risultante (sana o cancerosa). *count / 278* sarebbe il numero di cellule vicine ma siccome in questo caso siamo in una dimensione assumiamo quel numero (ç).\n",
    "\n",
    "    *   ``` python\n",
    "        if len(res) > 2:\n",
    "            if res[2] == 0:\n",
    "                to_add.append(HealthyCell(0))\n",
    "            elif res[2] == 1:\n",
    "                to_add.append(CancerCell(0))\n",
    "        ```\n",
    "        Se *res* contiene più di due elementi, significa che la cellula si è divisa. Se *res[2]* è 0, una nuova cellula sana viene aggiunta alla lista to_add. Se *res[2]* è 1, una nuova cellula cancerosa viene aggiunta alla lista to_add.\n",
    "\n",
    "    *   ``` python\n",
    "        self.glucose -= res[0]\n",
    "        self.oxygen -= res[1]\n",
    "        ```\n",
    "\n",
    "        La quantità di glucosio e ossigeno nell'ambiente viene ridotta in base al consumo delle cellule (*res[0]* e *res[1]* rispettivamente).\n",
    "    \n",
    "    *   ``` python\n",
    "        self.cells = [cell for cell in self.cells if cell.alive] + to_add\n",
    "        ```\n",
    "        L'elenco delle cellule viene aggiornato per includere solo le cellule che sono ancora vive (*cell.alive*) e per aggiungere le nuove cellule formate durante il ciclo (*to_add*).\n",
    "\n",
    "* **Riempimento delle risorse**:\n",
    "    ``` python\n",
    "    def fill_sources(self):\n",
    "    ```\n",
    "    La funzione fill_sources() aggiunge una quantità fissa di glucosio (13000) e ossigeno (450000) all'ambiente. Questo simula l'apporto costante di risorse necessarie per la sopravvivenza delle cellule.\n",
    "\n",
    "\n",
    "* **Avanzamento del tempo**: \n",
    "    ``` python\n",
    "    def go(self, ticks=1):\n",
    "    ```\n",
    "    La funzione è responsabile dell'avanzamento del tempo nell'ambiente cellulare e dell'aggiornamento dello stato delle risorse e delle cellule.\n",
    "\n",
    "    *   *ticks*: Un intero che rappresenta il numero di unità di tempo (ore) per cui avanzare la simulazione. Il valore predefinito è 1\n",
    "\n",
    "    * self.time += 1\n",
    "    Ogni iterazione del ciclo incrementa la variabile time di 1, rappresentando un'ora che passa nella simulazione.\n",
    "    \n",
    "    *   ``` python\n",
    "        self.cycle_cells()\n",
    "        ```\n",
    "        La funzione cycle_cells() aggiorna lo stato di ogni cellula nell'ambiente. Ogni cellula consuma risorse (glucosio e ossigeno) e può dividersi (mitosi) se le condizioni lo permettono. Le cellule morte vengono rimosse dalla lista self.cells e le nuove cellule vengono aggiunte.\n",
    "\n",
    "    *   ``` python\n",
    "        if self.draw_graph:\n",
    "        ```\n",
    "        Se l'attributo draw_graph è True, vengono aggiornati i dati per il grafico. I seguenti dati vengono aggiunti alle rispettive liste: *self.ticks*: La lista dei tempi (ore), *self.ccell_counts*: La lista dei conteggi delle cellule cancerose e  *self.hcell_counts*: La lista dei conteggi delle cellule sane.\n",
    "\n",
    "* **Irradiazione**:\n",
    "    ``` python\n",
    "    def irradiate(self, dose):\n",
    "    ```\n",
    "    Applica una dose di radiazione a tutte le cellule, determinando la loro sopravvivenza o morte in base alla dose ricevuta.\n",
    "\n",
    "* **Osservazione**:\n",
    "    ``` python\n",
    "    def observe(self):\n",
    "        return HealthyCell.cell_count, CancerCell.cell_count\n",
    "    ```\n",
    "    Ritorna il conteggio delle cellule sane e cancerose.\n",
    "\n",
    "* **Azione**:\n",
    "    La funzione act applica una dose di radiazione (determinata dall'azione) e avanza il tempo di 24 unità, ritornando la ricompensa aggiustata in base alla dose e agli effetti sulle cellule. 24 unità corrispondono a 24 ore (ç)\n",
    "    * Calcolo e Ritorno della Ricompensa\n",
    "        ``` python\n",
    "        return self.adjust_reward(dose, pre_ccell - post_ccell, pre_hcell - min(post_hcell, m_hcell))\n",
    "        ```\n",
    "        Vedi punto seguente\n",
    "\n",
    "* **Aggiustamento della ricompensa**\n",
    "    ``` python\n",
    "    def adjust_reward(self, dose, ccell_killed, hcells_lost):\n",
    "    ```\n",
    "    Calcola la ricompensa in base alla dose somministrata, al numero di cellule cancerose uccise e al numero di cellule sane perse. Di seguito sono riportate le formule per le ricompense:\n",
    "\n",
    "    * Se lo stato è terminale:\n",
    "        * *Se self.end_type* è *'L'* o *'T'*:\n",
    "        $$R = -1$$\n",
    "\n",
    "        * Altrimenti:\n",
    "            * Se *self.reward* è *'dose'*:\n",
    "            $$R = -\\frac{\\text{dose}}{200} + 0.5 + \\frac{\\text{HealthyCell.cell\\_count}}{4000}$$\n",
    "\n",
    "            * Altrimenti:\n",
    "            $$R = 0.5 + \\frac{\\text{HealthyCell.cell\\_count}}{4000}$$\n",
    "\n",
    "    * Se lo stato non è terminale: \n",
    "        * Se *self.reward* è *'dose'* oppure *'oar'*:\n",
    "        $$R = -\\frac{\\text{dose}}{200} + \\frac{\\text{ccell\\_killed} - 5.0 \\times \\text{hcells\\_lost}}{100000}$$\n",
    "\n",
    "        * Se *self.reward* è *'killed'*:\n",
    "        $$R = \\frac{\\text{ccell\\_killed} - \\text{reward} \\times \\text{hcells\\_lost}}{100000}$$\n",
    "\n",
    "\n",
    "    Il fattore 5.0 indica che la perdita di cellule sane ha un peso negativo cinque volte maggiore rispetto all'uccisione delle cellule cancerose.\n",
    "    \n",
    "    La funzione *'killed'* corrisponde all’obiettivo di massimizzare il danno alle cellule cancerose riducendo al minimo il danno alle cellule sane,*'dose'* aggiunge un parametro, portando l’agente a trovare trattamenti con dosi totali più piccole, poiché gli effetti a lungo termine correlati a questa dose totale non sono modellati nella simulazione.\n",
    "\n",
    "    Nota: quando leggi il paper, Killed and dose fa riferimento a *'dose'* nel codice mentre Kelled corrisponde a *'killed'*. (§ Controlla formule reward nel codice e nel paper. C'è una differenza nelle formule che può essere dovuta al fatto che questo codice corrisponde al caso scalare)\n",
    "\n",
    "    \n",
    "* **Stato terminale**: \n",
    "    ``` python\n",
    "    def inTerminalState(self):\n",
    "    ```\n",
    "    Determina se l'ambiente è in uno stato terminale (vittoria, sconfitta, o tempo esaurito).\n",
    "\n",
    "* **Disegno del grafico**: \n",
    "    ``` python\n",
    "    def draw(self, title):\n",
    "    ```\n",
    "    Disegna un grafico dell'evoluzione delle cellule nel tempo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *TabularLearner*\n",
    "\n",
    "Questa classe implementa l'algoritmo di Q-learning tabulare per ottimizzare il trattamento radioterapico.\n",
    "\n",
    "* **Inizializzazione** \n",
    "\n",
    "    ``` python\n",
    "    def __init__(self, env, cancer_cell_stages, healthy_cell_stages, actions, state_type):\n",
    "    ```\n",
    "    * cancer_cell_stages e healthy_cell_stages rappresentano iL numero di stages (in ciascuno stage ci sarà un determinato numero di cellule)\n",
    "\n",
    "    * In questa parte vengono calcolati gi helper (*state_helper_hcells* e *state_helper_ccells*). Servono a determinare il modo in cui i conteggi continui di cellule sane e cancerose vengono convertiti in stati discreti nel contesto dell'algoritmo di Q-learning. In altre parole, queste variabili definiscono i confini degli intervalli o \"stages\" in cui il numero di cellule è suddiviso per creare uno spazio di stati gestibile e finito.\n",
    "        ``` python\n",
    "        if (state_type == 'o'): #log\n",
    "\t        self.state_helper_hcells = exp(log(3500.0) / (healthy_cell_stages - 2.0))\n",
    "\t        self.state_helper_ccells = exp(log(40000.0) / (cancer_cell_stages - 2.0))\n",
    "        else: # lin\n",
    "\t        self.state_helper_hcells = 3500.0 / (healthy_cell_stages - 2.0)\n",
    "\t        self.state_helper_ccells = 40000.0 / (cancer_cell_stages - 2.0)\n",
    "        ```\n",
    "        Nel caso lineare divido il numero massimo(?) di cellule per il numero di stages ottenendo così il numero di cellule in ciascuno di essi. Nel caso logaritmico, divido il logaritmo del numero di cellule per il numero di stage. Il logaritmo fornisce come risultato l'esponente della base, cioè: $$log_ea = x$$ $$e^x = a$$ Siccome non abbiamo bisogno dell'esponente ma del numero di cellule che si incrementano ad ogni stage sucessivo, è necessario  convertire il logaritmo utilizzando l'esponenziale (inverso del logaritmo naturale).\n",
    "\n",
    "        ***NOTA***: Il motivo per cui si sottrae 2 dal denominatore nel calcolo delle variabili è legato alla gestione degli stati discreti alle estremità dell'intervallo di conteggio delle cellule. Gli stati agli estremi (0 e massimo) sono spesso dedicati a rappresentare condizioni estreme, come assenza di cellule o massimo affollamento. Sottrarre 2 assicura che ci sia spazio sufficiente tra gli stati centrali, permettendo una più ampia copertura delle condizioni centrali. Inoltre, mantenere uno spazio adeguato tra gli stati intermedi è cruciale per garantire che l'algoritmo possa distinguere tra variazioni più piccole nel conteggio delle cellule, migliorando l'accuratezza delle decisioni dell'agente di reinforcement learning.\n",
    "\n",
    "* ***Discretizzazione dello stato*** (Stato --> Numero di cellule sane o cancerose):\n",
    "    ``` python\n",
    "    def ccell_state(self, count):\n",
    "        if (self.state_type == 'o'): # log (natural logatitm)\n",
    "            return min(self.cancer_cell_stages - 1, int(ceil(log(CancerCell.cell_count + 1) / log(self.state_helper_ccells))))\n",
    "        else:\n",
    "            return min(self.cancer_cell_stages - 1, int(ceil(CancerCell.cell_count / self.state_helper_ccells)))\n",
    "\n",
    "\n",
    "    def hcell_state(self, count):\n",
    "        if (self.state_type == 'o'): # log\n",
    "            return min(self.healthy_cell_stages - 1, ceil(log(max(HealthyCell.cell_count -8, 1) ) / log(self.state_helper_hcells)))\n",
    "        else:\n",
    "            return min(self.healthy_cell_stages - 1, int(ceil(max(HealthyCell.cell_count - 9, 0) / self.state_helper_hcells)))\n",
    "    ``` \n",
    "    Queste funzioni prendono  il numero di cellule (cancerose e sane) e le mappa a uno stato discreto. Questo stato è un indice che viene utilizzato per accedere alla tabella Q. In che modo? Divindendo il numero di cellule che si vuole discretizzare per l'helper (numero di cellule in ogni stage). Se, per esempio, il numero rapporto è di due, l'indice per quello stato (numero di cellule) sarà 2 (ç). In particolare:\n",
    "    * **ceil()**: Arrotonda per eccesso\n",
    "    * **int()**: Trasforma il numero in un interno siccome abbiamo bisogno di un indice discretizzato per lo stato. \n",
    "    * **min()**: Si assicura che lo stato risultante non ecceda il massimo numero di stati e che non si sfori l'indice massimo per gli stati.\n",
    "\n",
    "* ***Conversione dello stato***: \n",
    "    ``` python\n",
    "    def convert(self, obs):\n",
    "        return self.ccell_state(obs[1]), self.hcell_state(obs[0])\n",
    "    ```\n",
    "\n",
    "    Converte l'osservazione attuale in uno stato utilizzabile dal Q-learning:\n",
    "    * *obs*: È una tupla o lista che rappresenta lo stato corrente dell'ambiente, nel formato (numero_di_cellule_sane, numero_di_cellule_cancerose). Viene ottenuta dall'ambiente attraverso la funzione observe()\n",
    "    * *obs[0]* rappresenta il conteggio delle cellule sane, mentre *obs[1]* rappresenta il conteggio delle cellule cancerose\n",
    "\n",
    "* ***Allenamento***: \n",
    "    ``` python\n",
    "    def train(self, steps, alpha, epsilon, disc_factor):\n",
    "    ```\n",
    "    Allena l'agente per un numero specificato di passi, aggiornando la tabella Q in base alle esperienze raccolte. Durante l'addestramento, l'agente esplora l'ambiente, aggiornando la tabella Q per apprendere la politica ottimale per somministrare il trattamento (dose di radiazione) alle cellule cancerose.\n",
    "\n",
    "    *   ``` python\n",
    "        while steps > 0: \n",
    "        ```\n",
    "        Il ciclo di addestramento continua fino a quando ci sono ancora passi di addestramento disponibili.\n",
    "\n",
    "    *   ``` python\n",
    "        while not self.env.inTerminalState() and steps > 0: \n",
    "        ```\n",
    "        Questo ciclo interno continua fino a quando l'ambiente non raggiunge uno stato terminale (vittoria, sconfitta, o tempo esaurito) e ci sono ancora passi disponibili.\n",
    "\n",
    "    * La matrice ***Q*** viene aggiornata nel seguente modo: \n",
    "        $$Q(s, a) = (1 - \\alpha) \\cdot Q(s, a) + \\alpha \\cdot \\left( r + \\gamma \\cdot \\max_{a'} Q(s', a') \\right)$$\n",
    "        Dove:\n",
    "        * $Q(s, a)$ è il valore Q corrente per lo stato \\( s \\) e l'azione \\( a \\).\n",
    "        * $\\alpha$ è il tasso di apprendimento (learning rate), un valore compreso tra 0 e 1, che determina quanto il nuovo valore stimato influisce sul valore Q esistente.\n",
    "        * $r$ è la ricompensa immediata ricevuta dopo aver eseguito l'azione \\( a \\) nello stato \\( s \\).\n",
    "        * $\\gamma$ è il fattore di sconto (discount factor), anch'esso compreso tra 0 e 1, che misura quanto l'agente valuta le ricompense future rispetto a quelle immediate.\n",
    "        * $\\max_{a'} Q(s', a')$ è il valore Q massimo per il nuovo stato \\( s' \\) considerando tutte le azioni possibili, rappresentando la stima del miglior valore futuro raggiungibile.\n",
    "\n",
    "* ***Test***:\n",
    "    ``` python\n",
    "    def test(self, episodes, disc_factor, verbose=False, eval=False):\n",
    "    ```\n",
    "    Testa l'agente su un numero specificato di episodi, raccogliendo statistiche sulle performance.\n",
    "    * Inizializzazione delle Variabili\n",
    "        ``` python\n",
    "        sum_scores = 0.0\n",
    "        sum_error = 0.0\n",
    "        lengths_arr = []\n",
    "        fracs_arr = []\n",
    "        doses_arr = []\n",
    "        sum_w = 0\n",
    "        survivals_arr = []\n",
    "        ```\n",
    "        * *sum_scores*: Accumula la somma delle ricompense totali ottenute in ogni episodio.\n",
    "        * *sum_error*: Accumula l'errore quadratico medio per ogni episodio, utile per valutare la qualità della stima delle ricompense.\n",
    "        * l*engths_arr, fracs_arr, doses_arr*: Liste per raccogliere statistiche sugli episodi, come la lunghezza, il numero di frazioni di dose somministrate e la dose totale somministrata.\n",
    "        * *sum_w*: Conta il numero di episodi che terminano con una vittoria ('W').\n",
    "        * *survivals_arr*: Raccoglie il tasso di sopravvivenza delle cellule sane per ogni episodio.\n",
    "    * Esecuzione degli Episodi di Test:\n",
    "        ``` python\n",
    "        for _ in range(episodes):\n",
    "            self.env.reset()\n",
    "            sum_r = 0\n",
    "            err = 0.0\n",
    "            fracs = 0\n",
    "            doses = 0\n",
    "            time = 0\n",
    "            init_hcell = HealthyCell.cell_count\n",
    "        ```\n",
    "        * *self.env.reset()*: Reimposta l'ambiente all'inizio di ogni episodio, garantendo che le condizioni iniziali siano sempre le stesse.\n",
    "        * *sum_r*: Somma delle ricompense ottenute nell'episodio corrente.\n",
    "        * *err*: Errore quadratico accumulato durante l'episodio.\n",
    "        * *fracs*: Conta il numero di frazioni di dose applicate.\n",
    "        * *doses*: Somma delle dosi somministrate.\n",
    "        * *time*: Tiene traccia del tempo trascorso in ore (ogni iterazione incrementa di 24 ore).\n",
    "        * *init_hcell*: Memorizza il numero iniziale di cellule sane.\n",
    "\n",
    "    * Simulazione dell'Episodio\n",
    "\n",
    "    * Raccolta delle Statistiche:\n",
    "        * Errore quadratico medio\n",
    "        $$err += (r + disc_factor * self.Q[new_state][action] - self.Q[state][action]) ** 2.0$$\n",
    "\n",
    "    * Valutazione dell'Episodio\n",
    "\n",
    "* ***Esecuzione***: \n",
    "``` python\n",
    "def run(self, n_epochs, train_steps, test_steps, init_alpha, alpha_mult, init_epsilon, final_epsilon, disc_factor):\n",
    "``` \n",
    "Esegue l'allenamento e il test dell'agente per un numero specificato di epoche, adattando i parametri di apprendimento.\n",
    "\n",
    "* ***Salvataggio e caricamento della tabella Q***: \n",
    "``` python\n",
    "def save_Q(self, name):\n",
    "    np.save(name, self.Q, allow_pickle=False)\n",
    "\n",
    "def load_Q(self, name):\n",
    "    self.Q = np.load(name+'.npy', allow_pickle=False)\n",
    "``` \n",
    "Salva e carica la tabella Q da un file.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
